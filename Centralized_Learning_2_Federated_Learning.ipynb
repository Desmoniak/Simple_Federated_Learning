{"cells":[{"cell_type":"markdown","metadata":{"id":"zQzPkLf52wz5"},"source":["# Centralized Learning to Federated Learning\n","\n","COGONI Guillaume (p1810070)"]},{"cell_type":"markdown","metadata":{"id":"YmABCOwjiMig"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5K8q71r_6ek"},"outputs":[],"source":["# Torch\n","import torch\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torch.optim import Adam, SGD\n","import torch.nn as nn\n","\n","# Time\n","import time\n","\n","# Random\n","import random\n","\n","# Tqdm\n","from tqdm import tqdm\n","from tqdm import trange\n","\n","# Copy\n","import copy\n","\n","# Sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n","\n","# Pandas\n","import pandas as pd\n","\n","# Matplotlib\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ke55SHU_3RGA"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"IgQbZsPziGUs"},"source":["# Importation of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzD-s_7RcUrx"},"outputs":[],"source":["dataset = pd.read_csv(\"./202207-divvy-tripdata.csv\") # Replace by your dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICT7hkh1Pvto"},"outputs":[],"source":["data = dataset.copy() # Copy"]},{"cell_type":"markdown","metadata":{"id":"CbVOH6qIiDUy"},"source":["# Visiualisation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1680168680881,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"Zt4JnQTWLkiE","outputId":"3622a692-3175-47ac-93fe-f49958014317"},"outputs":[],"source":["# Print columns, dtypes and the 3 first observation\n","print(data.columns)\n","print(data.dtypes)\n","data.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3179,"status":"ok","timestamp":1680168684047,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"yGs_s7U7MN4T","outputId":"07d40e8d-3b47-4344-d2d9-5a3f915e5e09"},"outputs":[],"source":["# Check if there is missing values and how many values are in the dataset\n","print(data.isna().sum().sort_values(ascending=False))\n","#print(\"---------------------------------------------\")\n","#print(data.count().sort_values(ascending=True))"]},{"cell_type":"markdown","metadata":{"id":"KsBVpdC3ybAh"},"source":["# Cleaning the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWfUXOI3ARLp"},"outputs":[],"source":["'''\n","@Param value: a string value with this format \"Hour(s):Minute(s):Second(s)\" \n","'''\n","def convert_to_seconds(value):\n","    if not any(char.isdigit() for char in value):\n","        return pd.NaT\n","    hour, minute, second = map(int, value.split(':'))\n","    if hour >= 15:\n","        hour = 0\n","    if minute >= 60:\n","        minute = 0\n","        hour += 1\n","    if second >= 60:\n","        second = 0\n","        minute += 1\n","    return hour*3600+minute*60+second\n","\n","data_prep = data.copy()\n","\n","# Convert the Series \"ride_length\" to second\n","data_prep[\"ride_length\"] = data_prep[\"ride_length\"].apply(convert_to_seconds)\n","\n","# dropNaN\n","data_prep = data_prep.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9894,"status":"ok","timestamp":1680168707095,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"imz1Blw1i5Kn","outputId":"ef460c24-0eec-4a92-d22e-129a07b2e079"},"outputs":[],"source":["# Check if there is missing values and how many values are in the dataset\n","print(data_prep.isna().sum().sort_values(ascending=False))\n","print(\"---------------------------------------------\")\n","print(data_prep.count().sort_values(ascending=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4274,"status":"ok","timestamp":1680168711341,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"d8jW9AfblNp0","outputId":"b2e2ef4a-51f1-40ab-d3a5-ed8ab28ed24a"},"outputs":[],"source":["# Check if ride_length correctly change\n","data_prep['ride_length'].sort_values(ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680168711342,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"5FDsBZ1SPU6P","outputId":"8d4921bd-9fa2-4f0f-d00f-8c3b23e50372"},"outputs":[],"source":["data_prep['ride_length'] = data_prep['ride_length'].astype('int64')\n","print(data.shape)\n","data_prep.dtypes"]},{"cell_type":"markdown","metadata":{"id":"OyCrsV-nyG6j"},"source":["# Selection of the features and the target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKWoPVk_l5IH"},"outputs":[],"source":["features = ['start_lat', \n","            'start_lng',\n","            'end_lat',\n","            'end_lng',\n","            'member_casual',\n","            'ride_length',\n","            'day_of_week']\n","            \n","target = [\"rideable_type\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPnjeGjryFiY"},"outputs":[],"source":["data_prep = data_prep.loc[(data_prep[\"rideable_type\"] == \"classic_bike\") | (data_prep[\"rideable_type\"] == \"electric_bike\")]\n","X = data_prep[features]\n","y = data_prep[target]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"elapsed":2856,"status":"ok","timestamp":1680168715342,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"qNP9poeulaPH","outputId":"55950957-bc72-49b4-dfd1-855ff5786f0a"},"outputs":[],"source":["plt.hist(y, bins=3)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68,"status":"ok","timestamp":1680168715394,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"2oU9IQXMoNtK","outputId":"ab6cee4b-d425-47f8-96a1-408b81806d8c"},"outputs":[],"source":["print(\"Information on X (shape and dtype)\")\n","print(X.shape)\n","print(X.dtypes)\n","print(\"\\n\")\n","print(\"Information on y (shape and dtype)\")\n","print(y.shape)\n","print(y.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"_vDt-kH5yRW9"},"source":["# Preprocessing on the data "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1758,"status":"ok","timestamp":1680168717090,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"qyqJ3rTtdSNc","outputId":"f19569e3-0280-4150-b2a4-d37191f3ebf0"},"outputs":[],"source":["y_OE = OrdinalEncoder().fit_transform(y)\n","\n","num_cols = [\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\", \"day_of_week\", \"ride_length\"]\n","categorical_cols_less_values = [\"member_casual\"]\n","\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), num_cols),\n","        ('cat_less_values', OrdinalEncoder(), categorical_cols_less_values)\n","    ])\n","\n","X_prep = preprocessor.fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"Cg0Ehe5OyhVK"},"source":["# Creation of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8YC0XrXldBv"},"outputs":[],"source":["X_tensor = torch.tensor(X_prep, dtype=torch.float32)\n","y_tensor = torch.tensor(y_OE, dtype=torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1680168717091,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"3kVIaxBPs4d4","outputId":"d8cd1df0-8495-4d91-8125-dbd51a913bf2"},"outputs":[],"source":["print(X.shape)\n","print(X_prep.shape)\n","print(X_tensor.shape)\n","\n","print(y_OE.shape)\n","print(y_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwBwCwkAktD-"},"outputs":[],"source":["dataset_tensor = torch.utils.data.TensorDataset(X_tensor, y_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1680168717093,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"n4h62iLoX7Lm","outputId":"dfdedb28-9b5e-454a-c82f-b8933476d368"},"outputs":[],"source":["len(dataset_tensor)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uTvNYt_E3bY7"},"source":["# Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yb4vOM0knpa"},"outputs":[],"source":["class MyNet(nn.Module):\n","    def __init__(self, _Input, _Output):\n","        super(MyNet, self).__init__()\n","        self.fc1 = nn.Linear(_Input, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, _Output)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"vONmDCrW2-Tg"},"source":["# Centralized Version"]},{"cell_type":"markdown","metadata":{"id":"RYD78fvtynOa"},"source":["## Separation of the data (train and validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jN1YLjf-lFRg"},"outputs":[],"source":["train_size = int(0.6 * len(dataset_tensor))\n","val_size = len(dataset_tensor) - train_size\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset_tensor, [train_size, val_size])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKd3zOzHzYce"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=3200, shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=3200, shuffle=True)\n","\n","dataloaders = {'train': train_loader, 'val': val_loader}\n","dataset_sizes= {'train': len(train_dataset), 'val': len(val_dataset)}"]},{"cell_type":"markdown","metadata":{"id":"6moJwxY8yxVX"},"source":["# Train and Test function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnRtxSoystRU"},"outputs":[],"source":["def train_and_test_nn(model, criterion, optimizer, dataloaders, batch_size, num_epochs=25):\n","\n","    since = time.time()\n","    # Instantiate the neural network and the optimizer\n","    model = model\n","    optimizer = optimizer\n","    criterion = criterion\n","    best_acc_avg = 0.0\n","\n","    # Train the neural network\n","    for epoch in range(num_epochs):\n","        print(\"\\n\")\n","        print(\"_________________________Epoch %d / %d ____________________\" % (epoch+1, num_epochs))\n","        print(\"\\n\")\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            correct = 0\n","            precision = 0.0\n","            recall = 0.0\n","            i = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    running_loss += loss.item()\n","                    correct += torch.sum(preds == labels.squeeze())\n","            \n","                # backward + optimize only if in training phase\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","                i= i+1\n","            \n","            ##Statistics\n","\n","            # Calculate the average loss\n","            loss_avg = running_loss / (i+1)\n","\n","            # Calculate the average accuracy\n","            accuracy_avg = correct.double() / dataset_sizes[phase]\n","\n","            # Print the average loss, accuracy, precision, recall for once for train and val per epoch\n","            print('PHASE %s:  [AVG loss: %.3f || AVG Accuracy: %.4f] ' % \n","                (phase, loss_avg, accuracy_avg))\n","            \n","\n","            # deep copy the model\n","            if phase == 'val' and accuracy_avg > best_acc_avg:\n","                best_acc_avg = accuracy_avg\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    time_elapsed = time.time() - since\n","    print(\"\\n\")\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc_avg))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"UmhN9Bsxy2tw"},"source":["# Starting training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":147286,"status":"error","timestamp":1680168865084,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"YM7fGFZ4zrtl","outputId":"55335630-52a5-4fdd-8968-c8b0904f64ba"},"outputs":[],"source":["model = MyNet(X_tensor.shape[1], y_tensor.shape[1])\n","epochs = 20\n","model = model.to(device)\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer_ft = Adam(model.parameters(), lr=0.001)\n","\n","#model_ft = train_and_test_nn(model, criterion, optimizer_ft, dataloaders, 1, num_epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"fory6wfo3EN6"},"source":["# Federated Version"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1680176103246,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"IGhS4eNvL89X"},"outputs":[],"source":["global_model = MyNet(X_tensor.shape[1], y_tensor.shape[1])\n","local_model = MyNet(X_tensor.shape[1], y_tensor.shape[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680176308652,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"BIqg10_hSKSE"},"outputs":[],"source":["def params_models_equals(model_1,model_2):\n","    \"\"\"_summary_\n","\n","    Args:\n","        model_1 (_type_): a neural network pythorch\n","        model_2 (_type_): a neural network pytorch\n","    \"\"\"\n","    \n","    # Vérifier si les deux modèles ont les mêmes paramètres\n","    params_1 = model_1.state_dict()\n","    params_2 = model_2.state_dict()\n","\n","    equal = all([torch.allclose(params_1[key], params_2[key]) for key in params_1.keys()])\n","\n","    if equal:\n","        print(\"The models have the same parameters.\")\n","    else:\n","        print(\"The models have differents parameters.\")"]},{"cell_type":"markdown","metadata":{"id":"54ojkDxt4FTt"},"source":["## Creation Nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1680176256996,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"Z3PQh3ol4JGt"},"outputs":[],"source":["node_1_data, node_2_data, node_3_data, node_4_data, _ = torch.utils.data.random_split(dataset_tensor, [10000, 10000, 10000, 10000, len(dataset_tensor) - 40000])\n","#node_1_data, node_2_data, _ = torch.utils.data.random_split(dataset_tensor, [300000, 300000, len(dataset_tensor) - 600000])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Splitting node_i_data into train_data, valid_data and test_data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680176256998,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"wACj8aYQ4cap"},"outputs":[],"source":["nodes = {\n","        'node_1': {'model': copy.deepcopy(local_model), 'data': node_1_data},\n","        'node_2': {'model': copy.deepcopy(local_model), 'data': node_2_data},\n","        'node_3': {'model': copy.deepcopy(local_model), 'data': node_3_data},\n","        'node_4': {'model': copy.deepcopy(local_model), 'data': node_4_data}\n","}\n","\"\"\" nodes = {\n","        'node_1': {'model': copy.deepcopy(local_model), 'data': node_1_data},\n","        'node_2': {'model': copy.deepcopy(local_model), 'data': node_2_data},\n","\n","} \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_valid_test(dataset, train_size=0.6, valid_size=0.2, test_size=0.2):\n","    return torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_data_nodes(nodes):\n","    \"\"\"_summary_\n","\n","    Args:\n","        nodes (_type_):\n","        {\n","            'node_1' :\n","            {\n","                model: #, \n","                data : #\n","            },\n","            'node_2' :\n","            {\n","                model: #, \n","                data : #\n","            }, ...\n","        }\n","\n","    Returns:\n","        _type_: a new dictionnary of nodes with a new format \n","        {\n","            'node_1' :\n","            {\n","                model: #, \n","                data : \n","                {\n","                    'train_data': #,\n","                    'valid_data': #,\n","                    'test_data': #\n","                    \n","                }\n","            }, ...\n","        }\n","    \"\"\"\n","    for node in nodes.keys():\n","        train_dataset, valid_dataset, test_dataset = train_valid_test(nodes[node]['data'], 0.6, 0.2, 0.2)\n","        nodes[node]['data'] = {'train_data': train_dataset, 'valid_data': valid_dataset, 'test_data': test_dataset}\n","    return nodes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nodes = split_data_nodes(nodes)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Copying global model parameters into local model of nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680176257778,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"PM71bdbUS7At","outputId":"d0ff7a44-5c41-45b4-b740-d80101abf008"},"outputs":[],"source":["\"\"\" for node in nodes.keys():\n","  params_models_equals(nodes[node]['model'], global_model) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":321,"status":"ok","timestamp":1680176299666,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"GZAqnlwGLMQ3"},"outputs":[],"source":["def send_global_model_to_node(global_model, node):\n","        \"\"\"Send the parameters of the global model to a local model of a node\n","\n","        Args:\n","            global_model (_type_): _description_\n","            node (_type_): {'model': #, 'train_data': #, 'valid_data': #, \"test_data\": #}\n","\n","        Returns:\n","            _type_: _description_\n","        \"\"\"\n","        node['model'].load_state_dict(copy.deepcopy(global_model.state_dict()))\n","        return node"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680176299939,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"azFzNwcGQmAJ"},"outputs":[],"source":["\"\"\" # We send the main model to the nodes. \n","for node in nodes.keys():\n","    nodes[node] = send_global_model_to_node(global_model, nodes[node]) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1680176301086,"user":{"displayName":"Guillaume Cogoni","userId":"11047458042820676389"},"user_tz":-120},"id":"8x0y1aAvRWTH","outputId":"290b7b65-d291-4c03-bccd-964cb2778838"},"outputs":[],"source":["\"\"\" for node in nodes.keys():\n","  params_models_equals(nodes[node]['model'], global_model) \"\"\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Function to Selection nodes that will train the global model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def selection_nodes(nb_nodes, nodes):\n","    \"\"\"_summary_\n","\n","    Args:\n","        nb_nodes (_type_): _description_\n","        nodes (_type_): \n","        {\n","            'node_1' :\n","            {\n","                model: #, \n","                data : #\n","            },\n","            'node_2' :\n","            {\n","                model: #, \n","                data : #\n","            }, ...\n","        }\n","\n","    Returns:\n","        _type_: a list of node_name that will participate to the training. \n","    \"\"\"\n","    nb_nodes_names = len(nodes.keys())\n","    if(nb_nodes > nb_nodes_names) : nb_nodes = nb_nodes_names\n","    elif(nb_nodes < 1) : nb_nodes = 1\n","    return random.sample(list(nodes.keys()), nb_nodes)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training node"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUkVwUvvUC3K"},"outputs":[],"source":["def train_and_test_node(node, criterion, optimizer, batch_size_train, batch_size_test, num_epochs=25):\n","    \"\"\"_summary_\n","\n","    Args:\n","        node (_type_): _description_\n","        criterion (_type_): _description_\n","        optimizer (_type_): _description_\n","        batch_size_train (_type_): _description_\n","        batch_size_test (_type_): _description_\n","        num_epochs (int, optional): _description_. Defaults to 25.\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    train_loader = torch.utils.data.DataLoader(node['data']['train_data'], batch_size=batch_size_train, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(node['data']['valid_data'], batch_size=batch_size_test, shuffle=True)\n","\n","    dataloaders = {'train': train_loader, 'val': val_loader}\n","    dataset_sizes= {'train': len(node['data']['train_data']), 'val': len(node['data']['train_data'])}\n","\n","    since = time.time()\n","    # Instantiate the neural network and the optimizer\n","    model = node['model']\n","    optimizer = optimizer\n","    criterion = criterion\n","    best_acc_avg = 0.0\n","\n","    #pbar = trange(num_epochs, unit=\"carrots\")\n","\n","    # Train the neural network\n","    for epoch in range(num_epochs):\n","        \"\"\" print(\"\\n\")\n","        print(\"_________________________Epoch %d / %d ____________________\" % (epoch+1, num_epochs))\n","        print(\"\\n\") \"\"\"\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            correct = 0\n","            i = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    running_loss += loss.item()\n","                    correct += torch.sum(preds == labels.squeeze())\n","\n","                # backward + optimize only if in training phase\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","                i+=1\n","            \n","            ##Statistics\n","\n","            # Calculate the average loss\n","            loss_avg = running_loss / (i+1)\n","\n","            # Calculate the average accuracy\n","            accuracy_avg = correct.double() / dataset_sizes[phase]\n","\n","            # Print the average loss, accuracy, precision, recall for once for train and val per epoch\n","            print('PHASE %s:  [AVG loss: %.3f || AVG Accuracy: %.4f] '% \n","                (phase, loss_avg, accuracy_avg))\n","            \n","\n","            # deep copy the model\n","            if phase == 'val' and accuracy_avg > best_acc_avg:\n","                best_acc_avg = accuracy_avg\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    time_elapsed = time.time() - since\n","    \"\"\" print(\"\\n\")\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc_avg)) \"\"\"\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return {'model': model, 'node_best_acc_avg': best_acc_avg}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test Node"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" nodes_best_avg = {}\n","\n","for node in nodes:\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = Adam(nodes[node]['model'].parameters(), lr=0.001)\n","    model_best_acc_avg = train_and_test_node(nodes[node], criterion, optimizer, 5000, 5000, num_epochs=10)\n","    nodes[node]['model'] = model_best_acc_avg['model']\n","    nodes_best_avg[node] = model_best_acc_avg['node_best_acc_avg']\n","print(nodes_best_avg) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def send_local_model_for_agg(global_model, nodes, node_in_training_mode):\n","    temp = node_in_training_mode.copy()\n","    named_params = nodes[temp[0]][\"model\"].named_parameters()\n","    state_dict = nodes[temp.pop(0)]['model'].state_dict()\n","    for name, param in named_params:\n","        for node in temp:\n","            state_dict[name] = state_dict[name] + nodes[node][\"model\"].state_dict()[name]\n","        state_dict[name] = state_dict[name]/(len(node_in_training_mode))\n","    global_model.load_state_dict(state_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" node_selectioned = selection_nodes(3, nodes)\n","global_model = send_local_model_for_agg(global_model, nodes, node_in_training_mode) \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plan_training_global_model(nodes, global_model, nb_round, nb_epoch, nb_nodes_selectioned):\n","        nodes_selectioned = selection_nodes(nb_nodes_selectioned, nodes)\n","        nodes_best_avg = {}\n","        node_before_after_agg = {}\n","\n","        # We send the main model to the selectioned nodes. \n","        for node in nodes_selectioned:\n","            send_global_model_to_node(global_model, nodes[node])\n","\n","        for k in range(nb_round):\n","            nodes_best_avg[k] = {}\n","            node_before_after_agg[k] = {}\n","            print(\"\\n\")\n","            print(\"############################################################\")\n","            print(\"_________________________Round %d / %d ____________________\" % (k+1, nb_round))\n","            print(\"############################################################\")\n","            print(\"\\n\")\n","            for node in nodes_selectioned:\n","                print(f\"_________________________TRAINING PHASE of {node}____________________\")\n","                criterion = nn.BCEWithLogitsLoss()\n","                optimizer = SGD(nodes[node]['model'].parameters(), lr=0.01)\n","                model_best_acc_avg = train_and_test_node(nodes[node], criterion, optimizer, 3200, 3200, num_epochs=nb_epoch)\n","                nodes_best_avg[k][node] = model_best_acc_avg['node_best_acc_avg']\n","\n","\n","            for node in nodes_selectioned:\n","                node_before_after_agg[k][node] = {\"before_agg\": test(nodes[node])}\n","            \n","            send_local_model_for_agg(global_model, nodes, nodes_selectioned)\n","\n","            for node in nodes_selectioned:\n","                send_global_model_to_node(global_model, nodes[node])\n","                node_before_after_agg[k][node][\"after_agg\"] = test(nodes[node])\n","\n","        for k in range(nb_round):\n","            print(\"_____________________________________________________________________\")\n","            print(f\"_________________________Results for round {k+1} ____________________\")\n","            print(\"_____________________________________________________________________\")\n","            for node in nodes_selectioned:\n","                print(f'Results for {node}')\n","                print(\"Best Accuracy\")\n","                print(nodes_best_avg[k][node])\n","                print(\"\\n\")\n","                print(\"Comparaison before and after aggregation\")\n","                print(node_before_after_agg[k][node])\n","                print(\"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plan_training_global_model(nodes,global_model, 4, 10, 4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOy1306ycc1JzxGaFFsJecU","mount_file_id":"1ECF1yeHjHF7wOFsJBUrO_WDcr0SGcMCu","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"vscode":{"interpreter":{"hash":"01aa32cd756f54750dc21a74163d5461327e720b8f79cf0f57336931cf0e5644"}}},"nbformat":4,"nbformat_minor":0}
